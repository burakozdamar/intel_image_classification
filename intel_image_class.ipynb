{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494d9701-f3d3-47c1-9691-4d3e2a34718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import pathlib\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c25135d-42e6-4213-b30d-b03ddfab9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a257c27f-9818-445d-bc69-9d71300763f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize= (150,150)\n",
    "# resize= (16,16)\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize(resize),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                         [0.5, 0.5, 0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498dccdb-0ed9-4d3c-8043-56267cfc28ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "batch_size = 32\n",
    "train_path, test_path = \"./seg_train/seg_train/\", \"./seg_test/seg_test/\"\n",
    "train_loader = DataLoader(torchvision.datasets.ImageFolder(train_path, transform=transformer), batch_size=batch_size, shuffle= True)\n",
    "test_loader = DataLoader(torchvision.datasets.ImageFolder(test_path, transform=transformer), batch_size=batch_size, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37062ff5-0807-4983-b55d-bb8a2e45c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = pathlib.Path(train_path)\n",
    "# classes = sorted([j.name for j in root.iterdir()])\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir() if not j.name.startswith(\".\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64e0fa5-a866-43a1-8d41-8a017cdba834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9919d822-c471-4cd4-9483-a899d5b060c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=6):\n",
    "        super(ConvNet, self).__init__()\n",
    "    # Input shape: \n",
    "    # batch_size, channels, image_size_height, image_size_width\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    #(256, 3, 150, 150)\n",
    "    \n",
    "    # Output shape: \n",
    "    # Output height = (Input height + padding height top + padding height bottom - kernel height) / (stride height) + 1\n",
    "    # Output width = (Output width + padding width right + padding width left - kernel width) / (stride width) + 1\n",
    "    #               (150+1+1-3)/1+1 = 150\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # (256, 12, 150, 150)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        # (256, 12, 150, 150)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # (256, 12, 150, 150)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        # Reduce the image size by kernel_size\n",
    "        # (256, 12, 75, 75)\n",
    "        \n",
    "        self.conv2= nn.Conv2d(in_channels=12, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "        # (256, 20, 75, 75)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # (256, 20, 75, 75)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # (256, 32, 75, 75)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        # (256, 32, 75, 75)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # (256, 32, 75, 75)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=32*resize[0]//2*resize[1]//2, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        output = self.conv1(inp)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "\n",
    "        output = output.view(-1, 32 * resize[0]//2 * resize[1]//2)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a073bad-e878-41f2-8e1c-89e6170eeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba5a163-fa93-4394-bf54-d7bee634779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),lr=0.001, weight_decay=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "num_epoch = 10\n",
    "train_count = len(glob.glob(train_path+'/*/*.jpg'))\n",
    "test_count = len(glob.glob(test_path+'/*/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "526d02e5-5a30-4ee0-8356-0f8e9c82318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smol/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/smol/repo/pytorch/c10/core/TensorImpl.h:1260.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Batch 50\n",
      "Batch 100\n",
      "Batch 150\n",
      "Batch 200\n",
      "Batch 250\n",
      "Batch 300\n",
      "Batch 350\n",
      "Batch 400\n",
      "Epoch: 0, train loss: 5.438430309295654, train acc: 0.5840102607952117, test acc 0.6743333333333333\n",
      "Batch 0\n",
      "Batch 50\n",
      "Batch 100\n",
      "Batch 150\n",
      "Batch 200\n",
      "Batch 250\n",
      "Batch 300\n",
      "Batch 350\n",
      "Batch 400\n",
      "Epoch: 1, train loss: 0.7532532811164856, train acc: 0.75502351432236, test acc 0.7366666666666667\n",
      "Batch 0\n",
      "Batch 50\n",
      "Batch 100\n",
      "Batch 150\n",
      "Batch 200\n",
      "Batch 250\n",
      "Batch 300\n",
      "Batch 350\n",
      "Batch 400\n",
      "Epoch: 2, train loss: 0.4815417230129242, train acc: 0.8415277183981759, test acc 0.735\n",
      "Batch 0\n",
      "Batch 50\n",
      "Batch 100\n",
      "Batch 150\n",
      "Batch 200\n",
      "Batch 250\n",
      "Batch 300\n",
      "Batch 350\n",
      "Batch 400\n",
      "Epoch: 3, train loss: 0.34936562180519104, train acc: 0.8857773977483255, test acc 0.7356666666666667\n",
      "Batch 0\n",
      "Batch 50\n",
      "Batch 100\n",
      "Batch 150\n",
      "Batch 200\n",
      "Batch 250\n",
      "Batch 300\n",
      "Batch 350\n",
      "Batch 400\n",
      "Epoch: 4, train loss: 0.2692165970802307, train acc: 0.917272338606242, test acc 0.718\n",
      "Batch 0\n",
      "Batch 50\n",
      "Batch 100\n",
      "Batch 150\n",
      "Batch 200\n",
      "Batch 250\n",
      "Batch 300\n",
      "Batch 350\n",
      "Batch 400\n",
      "Epoch: 5, train loss: 0.21934013068675995, train acc: 0.9309533988884139, test acc 0.761\n",
      "Batch 0\n",
      "Batch 50\n",
      "Batch 100\n",
      "Batch 150\n",
      "Batch 200\n",
      "Batch 250\n",
      "Batch 300\n",
      "Batch 350\n",
      "Batch 400\n",
      "Epoch: 6, train loss: 0.18123915791511536, train acc: 0.9440644149921619, test acc 0.7246666666666667\n",
      "Batch 0\n",
      "Batch 50\n",
      "Batch 100\n",
      "Batch 150\n",
      "Batch 200\n",
      "Batch 250\n",
      "Batch 300\n",
      "Batch 350\n",
      "Batch 400\n",
      "Epoch: 7, train loss: 0.16991554200649261, train acc: 0.9479834687188257, test acc 0.7586666666666667\n",
      "Batch 0\n",
      "Batch 50\n",
      "Batch 100\n",
      "Batch 150\n",
      "Batch 200\n",
      "Batch 250\n",
      "Batch 300\n",
      "Batch 350\n",
      "Batch 400\n",
      "Epoch: 8, train loss: 0.1656252145767212, train acc: 0.9494798346871882, test acc 0.7363333333333333\n",
      "Batch 0\n",
      "Batch 50\n",
      "Batch 100\n",
      "Batch 150\n",
      "Batch 200\n",
      "Batch 250\n",
      "Batch 300\n",
      "Batch 350\n",
      "Batch 400\n",
      "Epoch: 9, train loss: 0.16882522404193878, train acc: 0.9484110018526436, test acc 0.727\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    train_acc = 0.\n",
    "    test_acc = 0\n",
    "    train_loss = 0.\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if not i%50: print(f\"Batch {i}\")\n",
    "\n",
    "        train_loss += loss.cpu().data*images.size(0)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        train_acc += int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_acc = train_acc / train_count\n",
    "    train_loss = train_loss / train_count\n",
    "    \n",
    "    model.eval()\n",
    "    test_acc = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        test_acc += int(torch.sum(prediction==labels.data))\n",
    "    test_acc = test_acc / test_count\n",
    "    print(f\"Epoch: {epoch}, train loss: {train_loss}, train acc: {train_acc}, test acc {test_acc}\")\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        torch.save(model.state_dict(),\"best_checkpoint.model\")\n",
    "        best_acc= test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
